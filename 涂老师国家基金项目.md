# 转导推理

Transductive Inference，一种推理范式，学习完全为了解决眼前的特定问题，不需要泛化

> “你可以直接看考试题找出套路，而不用学完整本书”

**利用测试集信息**：训练时已看到测试集数据（无标签），用该数据的分布特征建模

**本质**：从特殊（已有的训练数据+未标注的测试数据）到特殊（测试集预测结果）



# 元学习

Meta-Learning，学习如何学习（如如何提取重要特征、如何快速更新参数），会接触**大量的小任务**

目标：得到一个**学习器**，面对**全新任务**时能快速学习

**本质**：从特殊（大量相关任务）到一般（一个通用的快速学习算法），再到特殊（快速解决新任务）





当然有！元学习（Meta-Learning）在深度学习领域是一个非常重要且活跃的方向，其核心思想是 **“学会学习”** 。它旨在训练模型掌握一种能力，使其能够利用以往任务中获得的知识或经验，快速适应新的、仅有少量样本的任务。

简单来说，传统的深度学习模型是为每个特定任务从头开始训练，而元学习模型则是被训练成一位“快速学习者”，在面对新任务时能迅速上手。

以下是元学习在深度学习中的几个主要应用领域，并附有具体例子：

### 1. 小样本学习

这是元学习最经典和成功的应用领域。

- **问题：** 在许多实际场景中（如医疗影像、罕见物体识别），收集大量标注数据非常困难或昂贵。
- **元学习方案：** 模型在大量不同的“小样本任务”上进行元训练。每个任务都只有一个支持集（少量样本）和一个查询集。模型学习的是如何根据支持集中的几个样本，快速识别查询集的样本。
- **应用实例：**
    - **图像分类：** 训练一个模型，使其在看到某个新物种的1-5张照片后，就能准确识别该物种。
    - **机器翻译：** 为低资源语言（数据量少的语言）快速构建翻译系统。模型从高资源语言的翻译经验中学习“如何学习翻译”，从而只需少量双语对照文本就能适应低资源语言。

### 2. 强化学习

元学习可以显著提升智能体在新环境中的适应效率。

- **问题：** 训练一个能在复杂环境中完成任务的强化学习智能体通常需要巨量的交互数据。如果环境发生微小变化，智能体可能就需要重新训练。
- **元学习方案：** 在多种不同的但相关的环境中训练智能体（例如，模拟机器人行走的不同地面摩擦力、不同坡度）。智能体学习的是一个**通用的策略调整机制**，而不是一个固定的策略。
- **应用实例：**
    - **机器人控制：** 一个在模拟器中学会行走的机器人，被部署到真实世界时，地面摩擦力和视觉输入都会变化。元学习可以让机器人通过短暂的试错，快速适应真实世界的物理特性，而不需要从头开始训练。
    - **游戏AI：** 训练一个AI玩一系列不同的游戏关卡或游戏规则，使其在遇到新关卡时能快速找到通关策略。

### 3. 超参数优化与神经网络结构搜索

自动化机器学习（AutoML）的核心任务。

- **问题：** 为每个新任务手动设计和调整模型超参数（如学习率、网络层数）非常耗时且需要专业知识。
- **元学习方案：** 在一个包含大量任务和其最优超参数的数据集上训练一个元模型。这个元模型学会预测：对于一个新的任务，什么样的超参数或网络结构会是高效的。
- **应用实例：**
    - **学习率规划器：** 元学习模型可以为一个新的训练任务预测出一组动态变化的学习率，而不是使用固定的或预设衰减的学习率，从而加速收敛。
    - **快速NAS：** 基于过去搜索网络结构的经验，元学习模型可以为一个新的数据集快速推荐一个有潜力的网络架构，大大减少搜索时间。

### 4. 快速微调与模型自适应

让大型预训练模型（如GPT、BERT）能更高效地适应下游任务。

- **问题：** 虽然预训练模型本身已经很强大了，但将其微调以适应特定任务（如情感分析、法律文本理解）仍然需要一定的数据和计算资源。
- **元学习方案：** 训练一个模型，使其内部的参数能够通过极小的改动（例如，只学习一组针对特定任务的“偏置”或“缩放”参数）来快速适应新任务。这种方法比完全微调所有参数更高效。
- **应用实例：**
    - **提示学习/前缀微调：** 可以看作一种元学习的思路，模型学习的是如何为不同任务生成合适的“提示”（Prompt），从而激活预训练模型本身的知识，而不是大幅度修改模型参数。

### 5. 持续学习/终生学习

让模型能够持续学习新知识而不遗忘旧知识。

- **问题：** 当一个神经网络按顺序学习多个任务时，学习新任务通常会严重干扰或覆盖掉为旧任务学到的权重，导致“灾难性遗忘”。
- **元学习方案：** 训练模型掌握一种“学习策略”，使其在学到新知识时，能够智能地决定哪些权重需要被保护（以防遗忘旧任务），哪些权重可以放心修改（以学习新任务）。
- **应用实例：**
    - **智能助手：** 一个智能助手需要不断学习用户的新习惯和偏好，同时又不能忘记如何执行基本的操作指令。

### 常见的元学习算法范式

为了理解这些应用，可以了解几种核心的元学习算法思想：

1. **基于优化的元学习：**
    - **核心思想：** 让模型的初始参数处于一个“绝佳”的起点，使得对于任何新任务，只需经过几步梯度下降就能达到良好性能。
    - **代表性算法：** **MAML**。
    - **比喻：** 不是教你解决具体问题，而是把你训练成一个“学霸”，让你面对一门新学科时，能自己快速找到学习方法并精通它。
2. **基于度量学习的元学习：**
    - **核心思想：** 学习一个通用的特征嵌入空间（Embedding Space），在这个空间里，相同类别的样本彼此靠近，不同类别的样本彼此远离。分类任务转化为在嵌入空间中最简单的最近邻搜索。
    - **代表性算法：** **孪生网络、匹配网络、原型网络**。
    - **比喻：** 学习一种“看人”的能力，让你能快速判断两张新面孔是否属于同一个人，而不是记住所有人的长相。
3. **基于模型结构的元学习：**
    - **核心思想：** 让模型内部包含一个“记忆单元”或“快速权重生成机制”，这个机制可以根据新任务的支持集动态地调整模型本身的行为。
    - **代表性算法：** **记忆增强神经网络**。

### 总结

| 应用领域           | 核心问题               | 元学习提供的价值                   |
| ------------------ | ---------------------- | ---------------------------------- |
| **小样本学习**     | 数据稀缺               | 从少量样本中快速学习和泛化         |
| **强化学习**       | 环境适应慢、样本效率低 | 获得快速适应新环境的能力           |
| **超参数优化/NAS** | 调参耗时、需要专家知识 | 将调参和结构搜索的经验迁移到新任务 |
| **模型快速微调**   | 大模型微调成本高       | 用极少的参数和数据实现任务自适应   |
| **持续学习**       | 灾难性遗忘             | 学会如何增量式学习而不遗忘         |

总而言之，元学习是推动人工智能向更通用、更灵活、更接近人类学习方式迈进的关键技术之一，尤其在数据稀缺、任务多变的现实世界中具有巨大的应用潜力。